** Config **
adapter: False
adapter_dim: None
adaptformer: True
adaptformer_scale: learnable
backbone: CLIP-ViT-B/16
batch_size: 128
bias_tuning: False
bn_tuning: False
classifier: CosineClassifier
dataset: CIFAR100_IR100
deterministic: True
expand: 24
full_tuning: False
gpu: 0
imb_factor: None
init_head: text_feat
ln_tuning: False
lora: False
lora_mlp: False
loss_type: LA
lr: 0.01
mask: False
mask_ratio: None
mask_seed: None
micro_batch_size: 128
model_dir: None
momentum: 0.9
num_epochs: 10
num_workers: 0
output_dir: ./output/cifar100_ir100_clip_vit_b16_adaptformer_True_gpu_0
partial: None
prec: amp
print_freq: 10
prompt: default
randaug_times: 1
resolution: 224
root: ./data
scale: 25
seed: 0
ssf_attn: False
ssf_ln: False
ssf_mlp: False
test_only: False
test_train: False
tte: False
tte_mode: fivecrop
vpt_deep: False
vpt_len: None
vpt_shallow: False
weight_decay: 0.0005
zero_shot: False
************
Setting fixed seed: 0
mean: [0.48145466, 0.4578275, 0.40821073]
std: [0.26862954, 0.26130258, 0.27577711]
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Total training points: 10847
Building model
Loading CLIP (backbone: CLIP-ViT-B/16)
Adapter bottle dimension set to 4
Turning off gradients in the model
Turning on gradients in the tuner
Turning on gradients in the head
Total params: 149798972
Tuned params: 101436
Head params: 76800
Initialize head with text features
Initialize tensorboard (log_dir=./output/cifar100_ir100_clip_vit_b16_adaptformer_True_gpu_0/tensorboard)
epoch [1/10] batch [10/85] time 0.895 (0.933) data 0.572 (0.540) loss 2.0904 (2.5956) acc 49.2188 (48.6463) (mean 34.8137 many 48.6354 med 37.7466 few 15.2667) lr 1.0000e-02 eta 0:13:03
epoch [1/10] batch [20/85] time 0.702 (0.897) data 0.379 (0.524) loss 1.8874 (2.1466) acc 55.4688 (52.6142) (mean 40.3577 many 55.3646 med 40.4435 few 22.7497) lr 1.0000e-02 eta 0:12:24
epoch [1/10] batch [30/85] time 0.705 (0.830) data 0.383 (0.472) loss 1.4959 (1.7966) acc 64.8438 (57.1471) (mean 44.7242 many 62.3058 med 42.7018 few 26.5717) lr 1.0000e-02 eta 0:11:20
epoch [1/10] batch [40/85] time 0.708 (0.818) data 0.385 (0.463) loss 1.6564 (1.6206) acc 57.0312 (58.3365) (mean 46.1699 many 61.1651 med 47.9614 few 26.5853) lr 1.0000e-02 eta 0:11:02
epoch [1/10] batch [50/85] time 0.688 (0.793) data 0.365 (0.441) loss 1.5001 (1.5279) acc 60.1562 (59.1096) (mean 46.4189 many 62.6409 med 48.3569 few 25.2322) lr 1.0000e-02 eta 0:10:34
epoch [1/10] batch [60/85] time 0.678 (0.777) data 0.308 (0.427) loss 1.2080 (1.4546) acc 64.0625 (60.0962) (mean 47.2182 many 60.4421 med 52.2740 few 25.8917) lr 1.0000e-02 eta 0:10:13
epoch [1/10] batch [70/85] time 0.690 (0.765) data 0.322 (0.414) loss 1.7376 (1.5076) acc 49.2188 (58.4508) (mean 45.8261 many 57.0006 med 51.6316 few 26.0160) lr 1.0000e-02 eta 0:09:56
epoch [1/10] batch [80/85] time 0.702 (0.756) data 0.379 (0.406) loss 1.3752 (1.4344) acc 65.6250 (59.8800) (mean 47.9292 many 60.8806 med 53.5532 few 26.2579) lr 1.0000e-02 eta 0:09:42
epoch [2/10] batch [10/85] time 0.672 (0.744) data 0.304 (0.397) loss 1.2764 (1.2850) acc 65.6250 (62.5845) (mean 51.2106 many 63.0223 med 57.8825 few 29.6464) lr 9.7553e-03 eta 0:09:21
epoch [2/10] batch [20/85] time 0.673 (0.741) data 0.304 (0.394) loss 1.4085 (1.1883) acc 60.1562 (65.0882) (mean 53.4669 many 67.7303 med 58.9598 few 30.4180) lr 9.7553e-03 eta 0:09:11
epoch [2/10] batch [30/85] time 0.893 (0.740) data 0.523 (0.392) loss 1.1105 (1.2209) acc 61.7188 (63.3683) (mean 51.6237 many 63.8943 med 57.1225 few 30.8928) lr 9.7553e-03 eta 0:09:04
epoch [2/10] batch [40/85] time 0.699 (0.738) data 0.333 (0.388) loss 1.3166 (1.2231) acc 63.2812 (64.3246) (mean 52.9973 many 64.0396 med 59.2740 few 32.7919) lr 9.7553e-03 eta 0:08:55
epoch [2/10] batch [50/85] time 0.690 (0.737) data 0.324 (0.386) loss 0.9756 (1.2447) acc 68.7500 (63.0653) (mean 53.4894 many 62.4893 med 60.9718 few 34.2601) lr 9.7553e-03 eta 0:08:46
epoch [2/10] batch [60/85] time 0.690 (0.734) data 0.324 (0.383) loss 1.1045 (1.2361) acc 60.1562 (63.1774) (mean 53.5490 many 62.9374 med 59.7339 few 35.3803) lr 9.7553e-03 eta 0:08:37
epoch [2/10] batch [70/85] time 0.683 (0.731) data 0.319 (0.380) loss 1.1931 (1.1949) acc 65.6250 (64.4408) (mean 55.2588 many 65.5559 med 60.8380 few 36.7364) lr 9.7553e-03 eta 0:08:28
epoch [2/10] batch [80/85] time 0.853 (0.739) data 0.529 (0.387) loss 0.9176 (1.1553) acc 73.4375 (66.3551) (mean 56.7534 many 67.7858 med 62.8239 few 36.7999) lr 9.7553e-03 eta 0:08:26
epoch [3/10] batch [10/85] time 0.703 (0.734) data 0.379 (0.384) loss 1.0651 (1.1223) acc 65.6250 (66.1465) (mean 57.1362 many 67.2102 med 62.5050 few 39.1197) lr 9.0451e-03 eta 0:08:11
epoch [3/10] batch [20/85] time 0.689 (0.732) data 0.324 (0.381) loss 1.0503 (1.0936) acc 64.8438 (65.5127) (mean 56.8494 many 64.8952 med 62.6402 few 40.7067) lr 9.0451e-03 eta 0:08:02
epoch [3/10] batch [30/85] time 0.694 (0.730) data 0.325 (0.379) loss 1.0665 (1.1246) acc 66.4062 (65.4619) (mean 57.4211 many 64.4105 med 63.1328 few 42.6032) lr 9.0451e-03 eta 0:07:54
epoch [3/10] batch [40/85] time 0.715 (0.728) data 0.391 (0.378) loss 1.1038 (1.0765) acc 64.8438 (66.5725) (mean 59.3188 many 68.4692 med 63.4706 few 43.7997) lr 9.0451e-03 eta 0:07:46
epoch [3/10] batch [50/85] time 0.690 (0.728) data 0.366 (0.379) loss 1.0693 (1.0998) acc 66.4062 (66.4918) (mean 59.8067 many 68.3082 med 63.9103 few 45.1007) lr 9.0451e-03 eta 0:07:38
epoch [3/10] batch [60/85] time 0.898 (0.729) data 0.523 (0.380) loss 0.8882 (1.1017) acc 67.9688 (65.8586) (mean 60.4358 many 66.8192 med 66.6906 few 45.6913) lr 9.0451e-03 eta 0:07:31
epoch [3/10] batch [70/85] time 0.856 (0.735) data 0.488 (0.386) loss 0.6993 (1.0908) acc 75.0000 (65.7133) (mean 60.1690 many 66.4649 med 66.4362 few 45.5120) lr 9.0451e-03 eta 0:07:28
epoch [3/10] batch [80/85] time 0.889 (0.741) data 0.519 (0.391) loss 1.0390 (1.0610) acc 66.4062 (66.0525) (mean 60.6723 many 66.4338 med 66.2127 few 47.4867) lr 9.0451e-03 eta 0:07:24
epoch [4/10] batch [10/85] time 0.917 (0.746) data 0.594 (0.396) loss 1.0714 (1.0271) acc 66.4062 (66.9786) (mean 61.8506 many 69.0465 med 65.6308 few 49.0450) lr 7.9389e-03 eta 0:07:16
epoch [4/10] batch [20/85] time 0.793 (0.751) data 0.421 (0.401) loss 0.8419 (1.0055) acc 74.2188 (68.7211) (mean 62.7321 many 70.8956 med 64.9936 few 50.5698) lr 7.9389e-03 eta 0:07:11
epoch [4/10] batch [30/85] time 0.697 (0.751) data 0.374 (0.402) loss 1.0299 (1.0122) acc 65.6250 (67.8343) (mean 62.1298 many 67.7733 med 66.2141 few 50.7807) lr 7.9389e-03 eta 0:07:04
epoch [4/10] batch [40/85] time 0.694 (0.749) data 0.370 (0.400) loss 1.2463 (1.0615) acc 64.0625 (66.4754) (mean 60.6334 many 65.1696 med 65.9851 few 49.0973) lr 7.9389e-03 eta 0:06:55
epoch [4/10] batch [50/85] time 0.662 (0.747) data 0.299 (0.399) loss 0.9712 (1.0146) acc 65.6250 (67.5868) (mean 60.7239 many 66.0781 med 65.0288 few 49.4549) lr 7.9389e-03 eta 0:06:47
epoch [4/10] batch [60/85] time 0.689 (0.745) data 0.365 (0.398) loss 0.6959 (0.9995) acc 81.2500 (68.0329) (mean 62.5490 many 68.1762 med 66.7222 few 51.1150) lr 7.9389e-03 eta 0:06:38
epoch [4/10] batch [70/85] time 0.687 (0.744) data 0.363 (0.397) loss 0.9921 (0.9732) acc 71.8750 (68.6271) (mean 63.3034 many 68.5457 med 68.3227 few 51.3317) lr 7.9389e-03 eta 0:06:30
epoch [4/10] batch [80/85] time 0.695 (0.743) data 0.371 (0.397) loss 0.9344 (0.9514) acc 74.2188 (69.2423) (mean 63.7005 many 69.0641 med 67.2514 few 53.3003) lr 7.9389e-03 eta 0:06:22
epoch [5/10] batch [10/85] time 0.697 (0.741) data 0.373 (0.396) loss 1.0650 (0.9840) acc 67.9688 (68.3515) (mean 64.7941 many 68.5685 med 70.1295 few 54.1658) lr 6.5451e-03 eta 0:06:10
epoch [5/10] batch [20/85] time 0.693 (0.740) data 0.369 (0.395) loss 0.8570 (0.9194) acc 71.0938 (70.4317) (mean 66.3296 many 69.9361 med 71.4965 few 56.0940) lr 6.5451e-03 eta 0:06:02
epoch [5/10] batch [30/85] time 0.693 (0.739) data 0.369 (0.394) loss 1.2726 (0.9544) acc 62.5000 (68.9588) (mean 64.6258 many 66.4270 med 69.8363 few 56.4455) lr 6.5451e-03 eta 0:05:54
epoch [5/10] batch [40/85] time 0.893 (0.741) data 0.520 (0.397) loss 0.9322 (0.9519) acc 68.7500 (68.6839) (mean 65.7311 many 66.6841 med 71.2047 few 58.2335) lr 6.5451e-03 eta 0:05:48
epoch [5/10] batch [50/85] time 0.894 (0.744) data 0.527 (0.399) loss 1.0145 (0.9811) acc 64.8438 (68.2315) (mean 65.8199 many 69.0242 med 70.0577 few 57.1373) lr 6.5451e-03 eta 0:05:42
epoch [5/10] batch [60/85] time 0.694 (0.744) data 0.370 (0.399) loss 0.9201 (0.9872) acc 71.0938 (67.8972) (mean 65.6393 many 68.1000 med 70.7642 few 56.7895) lr 6.5451e-03 eta 0:05:34
epoch [5/10] batch [70/85] time 0.695 (0.743) data 0.371 (0.399) loss 0.8822 (0.9681) acc 70.3125 (68.5055) (mean 65.2802 many 67.6891 med 68.6347 few 58.5564) lr 6.5451e-03 eta 0:05:27
epoch [5/10] batch [80/85] time 0.687 (0.742) data 0.362 (0.398) loss 1.1241 (0.9576) acc 63.2812 (68.9307) (mean 65.5447 many 66.3109 med 70.5040 few 58.8650) lr 6.5451e-03 eta 0:05:19
epoch [6/10] batch [10/85] time 0.688 (0.740) data 0.365 (0.397) loss 1.1694 (0.9686) acc 64.8438 (67.7870) (mean 65.0316 many 66.9056 med 68.0466 few 59.3278) lr 5.0000e-03 eta 0:05:07
epoch [6/10] batch [20/85] time 0.690 (0.739) data 0.366 (0.396) loss 0.9795 (0.9631) acc 67.9688 (68.6627) (mean 66.2857 many 68.9093 med 68.4372 few 60.7148) lr 5.0000e-03 eta 0:04:59
epoch [6/10] batch [30/85] time 0.694 (0.738) data 0.371 (0.396) loss 1.0749 (0.9286) acc 59.3750 (68.5916) (mean 66.0943 many 65.6449 med 70.7240 few 61.2174) lr 5.0000e-03 eta 0:04:51
epoch [6/10] batch [40/85] time 0.696 (0.737) data 0.372 (0.396) loss 1.0571 (0.9152) acc 69.5312 (69.8073) (mean 67.3869 many 67.9756 med 72.1256 few 61.1715) lr 5.0000e-03 eta 0:04:43
epoch [6/10] batch [50/85] time 0.697 (0.737) data 0.373 (0.395) loss 0.7623 (0.8553) acc 73.4375 (71.0639) (mean 68.1308 many 71.2662 med 70.0164 few 62.2729) lr 5.0000e-03 eta 0:04:36
epoch [6/10] batch [60/85] time 0.877 (0.737) data 0.507 (0.395) loss 0.8601 (0.9110) acc 68.7500 (69.8638) (mean 67.0530 many 69.0315 med 68.1176 few 63.5028) lr 5.0000e-03 eta 0:04:28
epoch [6/10] batch [70/85] time 0.693 (0.737) data 0.368 (0.396) loss 1.0140 (0.9316) acc 60.9375 (69.8679) (mean 66.1370 many 67.3346 med 66.0642 few 64.8245) lr 5.0000e-03 eta 0:04:21
epoch [6/10] batch [80/85] time 0.692 (0.736) data 0.368 (0.395) loss 1.2648 (0.9611) acc 63.2812 (69.0014) (mean 65.6976 many 66.7542 med 67.5487 few 62.3054) lr 5.0000e-03 eta 0:04:14
epoch [7/10] batch [10/85] time 0.693 (0.735) data 0.369 (0.394) loss 0.7993 (0.9169) acc 70.3125 (69.6082) (mean 67.6486 many 70.0250 med 69.8262 few 62.3357) lr 3.4549e-03 eta 0:04:02
epoch [7/10] batch [20/85] time 0.679 (0.734) data 0.355 (0.394) loss 0.7646 (0.9039) acc 73.4375 (70.3726) (mean 67.0936 many 70.4977 med 67.6580 few 62.4636) lr 3.4549e-03 eta 0:03:54
epoch [7/10] batch [30/85] time 0.689 (0.733) data 0.366 (0.394) loss 0.7540 (0.8714) acc 74.2188 (71.9047) (mean 68.4071 many 72.0300 med 70.4232 few 61.8284) lr 3.4549e-03 eta 0:03:47
epoch [7/10] batch [40/85] time 0.697 (0.733) data 0.374 (0.393) loss 0.9005 (0.8821) acc 69.5312 (70.9067) (mean 67.4916 many 70.2274 med 70.8140 few 60.4237) lr 3.4549e-03 eta 0:03:39
epoch [7/10] batch [50/85] time 0.690 (0.732) data 0.366 (0.393) loss 0.5530 (0.8416) acc 78.1250 (71.9281) (mean 68.5213 many 72.0046 med 70.3191 few 62.3600) lr 3.4549e-03 eta 0:03:32
epoch [7/10] batch [60/85] time 0.700 (0.731) data 0.376 (0.392) loss 0.8844 (0.8413) acc 67.9688 (71.7018) (mean 68.7986 many 70.6101 med 72.0099 few 62.9386) lr 3.4549e-03 eta 0:03:24
epoch [7/10] batch [70/85] time 0.730 (0.731) data 0.406 (0.392) loss 0.9429 (0.9062) acc 65.6250 (69.6451) (mean 67.0196 many 67.7263 med 68.4254 few 64.5551) lr 3.4549e-03 eta 0:03:17
epoch [7/10] batch [80/85] time 0.696 (0.730) data 0.372 (0.392) loss 0.8759 (0.9036) acc 72.6562 (70.0407) (mean 68.4852 many 69.6811 med 69.9797 few 65.3463) lr 3.4549e-03 eta 0:03:09
epoch [8/10] batch [10/85] time 0.903 (0.730) data 0.578 (0.392) loss 0.7868 (0.8601) acc 72.6562 (71.3276) (mean 69.1495 many 68.7985 med 71.8418 few 66.4182) lr 2.0611e-03 eta 0:02:58
epoch [8/10] batch [20/85] time 0.689 (0.731) data 0.365 (0.393) loss 0.8301 (0.8788) acc 68.7500 (69.4919) (mean 68.4847 many 69.1940 med 68.6954 few 67.4112) lr 2.0611e-03 eta 0:02:51
epoch [8/10] batch [30/85] time 0.684 (0.731) data 0.360 (0.393) loss 0.8655 (0.8602) acc 68.7500 (70.5132) (mean 69.0795 many 69.5583 med 69.5940 few 67.9207) lr 2.0611e-03 eta 0:02:44
epoch [8/10] batch [40/85] time 0.700 (0.731) data 0.376 (0.393) loss 0.8467 (0.8508) acc 74.2188 (70.7676) (mean 69.3354 many 70.0144 med 70.5372 few 67.1412) lr 2.0611e-03 eta 0:02:37
epoch [8/10] batch [50/85] time 0.691 (0.732) data 0.367 (0.394) loss 0.8229 (0.8399) acc 72.6562 (71.5136) (mean 69.7140 many 70.4669 med 71.4104 few 66.8564) lr 2.0611e-03 eta 0:02:30
epoch [8/10] batch [60/85] time 0.683 (0.731) data 0.319 (0.393) loss 0.7565 (0.8533) acc 71.8750 (72.1144) (mean 69.9022 many 70.5996 med 71.5935 few 67.1156) lr 2.0611e-03 eta 0:02:22
epoch [8/10] batch [70/85] time 0.681 (0.731) data 0.314 (0.393) loss 0.7044 (0.8416) acc 71.8750 (71.6136) (mean 70.4691 many 71.0434 med 73.9192 few 65.7740) lr 2.0611e-03 eta 0:02:15
epoch [8/10] batch [80/85] time 0.696 (0.731) data 0.371 (0.393) loss 0.7513 (0.8588) acc 75.7812 (71.2692) (mean 70.1640 many 71.0867 med 73.3053 few 65.4225) lr 2.0611e-03 eta 0:02:07
epoch [9/10] batch [10/85] time 0.897 (0.731) data 0.528 (0.393) loss 0.8404 (0.8505) acc 71.0938 (71.5481) (mean 70.7252 many 71.7640 med 73.3451 few 66.4567) lr 9.5492e-04 eta 0:01:57
epoch [9/10] batch [20/85] time 0.699 (0.732) data 0.375 (0.394) loss 0.9837 (0.8507) acc 67.9688 (71.4491) (mean 69.7187 many 70.7277 med 70.5446 few 67.5780) lr 9.5492e-04 eta 0:01:49
epoch [9/10] batch [30/85] time 0.687 (0.731) data 0.320 (0.393) loss 0.8407 (0.8281) acc 75.7812 (72.6870) (mean 70.7976 many 74.2589 med 70.8679 few 66.6775) lr 9.5492e-04 eta 0:01:42
epoch [9/10] batch [40/85] time 0.695 (0.731) data 0.371 (0.393) loss 0.7188 (0.8028) acc 74.2188 (73.3657) (mean 71.8162 many 73.2443 med 74.0737 few 67.5164) lr 9.5492e-04 eta 0:01:34
epoch [9/10] batch [50/85] time 0.692 (0.730) data 0.369 (0.393) loss 0.9162 (0.8369) acc 64.8438 (71.6233) (mean 69.9478 many 70.4815 med 71.2053 few 67.8579) lr 9.5492e-04 eta 0:01:27
epoch [9/10] batch [60/85] time 0.703 (0.730) data 0.380 (0.392) loss 1.2589 (0.8872) acc 62.5000 (69.9884) (mean 69.1048 many 67.4636 med 70.9709 few 68.8423) lr 9.5492e-04 eta 0:01:20
epoch [9/10] batch [70/85] time 0.718 (0.730) data 0.351 (0.392) loss 0.7978 (0.8695) acc 74.2188 (71.9185) (mean 70.7434 many 72.1821 med 70.4669 few 69.3874) lr 9.5492e-04 eta 0:01:12
epoch [9/10] batch [80/85] time 0.699 (0.729) data 0.375 (0.392) loss 0.8704 (0.8566) acc 72.6562 (72.0267) (mean 70.2905 many 71.5000 med 70.2132 few 68.9697) lr 9.5492e-04 eta 0:01:05
epoch [10/10] batch [10/85] time 0.707 (0.728) data 0.384 (0.391) loss 0.5929 (0.8404) acc 82.0312 (72.3222) (mean 70.3736 many 73.4061 med 68.5485 few 68.9647) lr 2.4472e-04 eta 0:00:54
epoch [10/10] batch [20/85] time 0.689 (0.728) data 0.366 (0.391) loss 0.6179 (0.8237) acc 76.5625 (72.3913) (mean 71.2995 many 74.2494 med 70.7571 few 68.4908) lr 2.4472e-04 eta 0:00:47
epoch [10/10] batch [30/85] time 0.686 (0.727) data 0.362 (0.390) loss 0.9300 (0.8092) acc 67.9688 (71.8655) (mean 70.0773 many 70.8833 med 71.0747 few 67.9732) lr 2.4472e-04 eta 0:00:40
epoch [10/10] batch [40/85] time 0.687 (0.727) data 0.363 (0.390) loss 0.8855 (0.8381) acc 70.3125 (72.2663) (mean 70.0860 many 72.3303 med 70.0691 few 67.4872) lr 2.4472e-04 eta 0:00:32
epoch [10/10] batch [50/85] time 0.696 (0.726) data 0.371 (0.390) loss 0.8230 (0.8581) acc 71.8750 (72.3396) (mean 70.7690 many 72.9649 med 71.2802 few 67.6106) lr 2.4472e-04 eta 0:00:25
epoch [10/10] batch [60/85] time 0.697 (0.726) data 0.372 (0.390) loss 0.9875 (0.8672) acc 67.1875 (71.2430) (mean 69.8855 many 69.6102 med 71.5820 few 68.2273) lr 2.4472e-04 eta 0:00:18
epoch [10/10] batch [70/85] time 0.694 (0.726) data 0.370 (0.389) loss 0.9662 (0.8751) acc 71.8750 (71.4879) (mean 71.0583 many 69.7254 med 73.3593 few 69.9287) lr 2.4472e-04 eta 0:00:10
epoch [10/10] batch [80/85] time 0.710 (0.726) data 0.385 (0.389) loss 0.7824 (0.7942) acc 74.2188 (72.8238) (mean 72.5816 many 71.3206 med 75.0744 few 71.1446) lr 2.4472e-04 eta 0:00:03
Finish training
Note that the printed training acc is not precise. To get precise training acc, use option ``test_train True``.
Time elapsed: 0:10:22
Checkpoint saved to ./output/cifar100_ir100_clip_vit_b16_adaptformer_True_gpu_0/checkpoint.pth.tar
Evaluate on the test set
=> result
* total: 10,000
* correct: 7,985
* accuracy: 79.8%
* error: 20.2%
* macro_f1: 79.6%
* class acc: [95. 95. 90. 74. 57. 93. 91. 73. 93. 95. 64. 78. 92. 86. 96. 91. 91. 90.
 75. 87. 91. 96. 88. 79. 93. 79. 71. 70. 88. 81. 78. 94. 72. 56. 85. 80.
 92. 86. 78. 89. 86. 86. 75. 89. 76. 73. 87. 45. 98. 92. 64. 85. 82. 95.
 95. 45. 96. 87. 94. 69. 87. 81. 88. 67. 60. 74. 75. 61. 98. 78. 84. 89.
 51. 65. 52. 86. 91. 88. 81. 80. 66. 79. 92. 80. 69. 83. 86. 90. 80. 97.
 90. 79. 58. 65. 88. 62. 33. 74. 43. 64.]
* worst_case_acc: 33.0%
* hmean_acc: 76.6%
* gmean_acc: 78.4%
* many: 83.6%  med: 80.4%  few: 74.8%
* average: 79.8%
