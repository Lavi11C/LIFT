** Config **
adapter: False
adapter_dim: None
adaptformer: True
adaptformer_scale: learnable
backbone: CLIP-ViT-B/16
batch_size: 128
bias_tuning: False
bn_tuning: False
classifier: CosineClassifier
dataset: CIFAR100_IR100
deterministic: True
expand: 24
full_tuning: False
gpu: 0
imb_factor: None
init_head: text_feat
ln_tuning: False
lora: False
lora_mlp: False
loss_type: LA
lr: 0.01
mask: False
mask_ratio: None
mask_seed: None
micro_batch_size: 128
model_dir: None
momentum: 0.9
num_epochs: 10
num_workers: 0
output_dir: ./output/cifar100_ir100_clip_vit_b16_adaptformer_True_gpu_0
partial: None
prec: amp
print_freq: 10
prompt: default
randaug_times: 1
resolution: 224
root: ./data
scale: 25
seed: 0
ssf_attn: False
ssf_ln: False
ssf_mlp: False
test_only: False
test_train: False
tte: False
tte_mode: fivecrop
vpt_deep: False
vpt_len: None
vpt_shallow: False
weight_decay: 0.0005
zero_shot: False
************
Setting fixed seed: 0
mean: [0.48145466, 0.4578275, 0.40821073]
std: [0.26862954, 0.26130258, 0.27577711]
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Total training points: 10847
Building model
Loading CLIP (backbone: CLIP-ViT-B/16)
Adapter bottle dimension set to 4
Turning off gradients in the model
Turning on gradients in the tuner
Turning on gradients in the head
Total params: 149798972
Tuned params: 101436
Head params: 76800
Initialize head with text features
Initialize tensorboard (log_dir=./output/cifar100_ir100_clip_vit_b16_adaptformer_True_gpu_0/tensorboard)
epoch [1/10] batch [10/85] time 0.817 (0.888) data 0.495 (0.492) loss 2.0904 (2.5956) acc 49.2188 (48.6463) (mean 34.8137 many 48.6354 med 37.7466 few 15.2667) lr 1.0000e-02 eta 0:12:26
epoch [1/10] batch [20/85] time 0.687 (0.880) data 0.362 (0.506) loss 1.8874 (2.1466) acc 55.4688 (52.6142) (mean 40.3577 many 55.3646 med 40.4435 few 22.7497) lr 1.0000e-02 eta 0:12:10
epoch [1/10] batch [30/85] time 0.831 (0.865) data 0.507 (0.500) loss 1.4959 (1.7966) acc 64.8438 (57.1471) (mean 44.7242 many 62.3058 med 42.7018 few 26.5717) lr 1.0000e-02 eta 0:11:49
epoch [1/10] batch [40/85] time 0.748 (0.837) data 0.400 (0.479) loss 1.6564 (1.6206) acc 57.0312 (58.3365) (mean 46.1699 many 61.1651 med 47.9614 few 26.5853) lr 1.0000e-02 eta 0:11:18
epoch [1/10] batch [50/85] time 0.692 (0.830) data 0.368 (0.476) loss 1.5001 (1.5279) acc 60.1562 (59.1096) (mean 46.4189 many 62.6409 med 48.3569 few 25.2322) lr 1.0000e-02 eta 0:11:03
epoch [1/10] batch [60/85] time 0.720 (0.818) data 0.396 (0.466) loss 1.2080 (1.4546) acc 64.0625 (60.0962) (mean 47.2182 many 60.4421 med 52.2740 few 25.8917) lr 1.0000e-02 eta 0:10:46
epoch [1/10] batch [70/85] time 0.893 (0.814) data 0.523 (0.463) loss 1.7376 (1.5076) acc 49.2188 (58.4508) (mean 45.8261 many 57.0006 med 51.6316 few 26.0160) lr 1.0000e-02 eta 0:10:35
epoch [1/10] batch [80/85] time 0.762 (0.818) data 0.399 (0.466) loss 1.3752 (1.4344) acc 65.6250 (59.8800) (mean 47.9292 many 60.8806 med 53.5532 few 26.2579) lr 1.0000e-02 eta 0:10:29
epoch [2/10] batch [10/85] time 0.728 (0.804) data 0.404 (0.455) loss 1.2764 (1.2850) acc 65.6250 (62.5845) (mean 51.2106 many 63.0223 med 57.8825 few 29.6464) lr 9.7553e-03 eta 0:10:07
epoch [2/10] batch [20/85] time 0.879 (0.809) data 0.506 (0.459) loss 1.4085 (1.1883) acc 60.1562 (65.0882) (mean 53.4669 many 67.7303 med 58.9598 few 30.4180) lr 9.7553e-03 eta 0:10:02
epoch [2/10] batch [30/85] time 0.694 (0.805) data 0.370 (0.457) loss 1.1105 (1.2209) acc 61.7188 (63.3683) (mean 51.6237 many 63.8943 med 57.1225 few 30.8928) lr 9.7553e-03 eta 0:09:51
epoch [2/10] batch [40/85] time 0.697 (0.798) data 0.372 (0.451) loss 1.3166 (1.2231) acc 63.2812 (64.3246) (mean 52.9973 many 64.0396 med 59.2740 few 32.7919) lr 9.7553e-03 eta 0:09:38
epoch [2/10] batch [50/85] time 0.896 (0.803) data 0.549 (0.456) loss 0.9756 (1.2447) acc 68.7500 (63.0653) (mean 53.4894 many 62.4893 med 60.9718 few 34.2601) lr 9.7553e-03 eta 0:09:34
epoch [2/10] batch [60/85] time 0.860 (0.801) data 0.496 (0.455) loss 1.1045 (1.2361) acc 60.1562 (63.1774) (mean 53.5490 many 62.9374 med 59.7339 few 35.3803) lr 9.7553e-03 eta 0:09:24
epoch [2/10] batch [70/85] time 0.689 (0.800) data 0.365 (0.454) loss 1.1931 (1.1949) acc 65.6250 (64.4408) (mean 55.2588 many 65.5559 med 60.8380 few 36.7364) lr 9.7553e-03 eta 0:09:15
epoch [2/10] batch [80/85] time 0.781 (0.797) data 0.455 (0.451) loss 0.9176 (1.1553) acc 73.4375 (66.3551) (mean 56.7534 many 67.7858 med 62.8239 few 36.7999) lr 9.7553e-03 eta 0:09:05
epoch [3/10] batch [10/85] time 0.885 (0.796) data 0.559 (0.452) loss 1.0651 (1.1223) acc 65.6250 (66.1465) (mean 57.1362 many 67.2102 med 62.5050 few 39.1197) lr 9.0451e-03 eta 0:08:53
epoch [3/10] batch [20/85] time 0.766 (0.799) data 0.420 (0.455) loss 1.0503 (1.0936) acc 64.8438 (65.5127) (mean 56.8494 many 64.8952 med 62.6402 few 40.7067) lr 9.0451e-03 eta 0:08:47
epoch [3/10] batch [30/85] time 0.793 (0.802) data 0.469 (0.458) loss 1.0665 (1.1246) acc 66.4062 (65.4619) (mean 57.4211 many 64.4105 med 63.1328 few 42.6032) lr 9.0451e-03 eta 0:08:41
epoch [3/10] batch [40/85] time 0.698 (0.799) data 0.370 (0.456) loss 1.1038 (1.0765) acc 64.8438 (66.5725) (mean 59.3188 many 68.4692 med 63.4706 few 43.7997) lr 9.0451e-03 eta 0:08:31
epoch [3/10] batch [50/85] time 0.691 (0.795) data 0.365 (0.452) loss 1.0693 (1.0998) acc 66.4062 (66.4918) (mean 59.8067 many 68.3082 med 63.9103 few 45.1007) lr 9.0451e-03 eta 0:08:20
epoch [3/10] batch [60/85] time 0.878 (0.795) data 0.509 (0.452) loss 0.8882 (1.1017) acc 67.9688 (65.8586) (mean 60.4358 many 66.8192 med 66.6906 few 45.6913) lr 9.0451e-03 eta 0:08:12
epoch [3/10] batch [70/85] time 0.758 (0.793) data 0.393 (0.451) loss 0.6993 (1.0908) acc 75.0000 (65.7133) (mean 60.1690 many 66.4649 med 66.4362 few 45.5120) lr 9.0451e-03 eta 0:08:03
epoch [3/10] batch [80/85] time 0.773 (0.791) data 0.448 (0.449) loss 1.0390 (1.0610) acc 66.4062 (66.0525) (mean 60.6723 many 66.4338 med 66.2127 few 47.4867) lr 9.0451e-03 eta 0:07:54
epoch [4/10] batch [10/85] time 0.697 (0.788) data 0.372 (0.447) loss 1.0714 (1.0271) acc 66.4062 (66.9786) (mean 61.8506 many 69.0465 med 65.6308 few 49.0450) lr 7.9389e-03 eta 0:07:40
epoch [4/10] batch [20/85] time 0.814 (0.786) data 0.489 (0.445) loss 0.8419 (1.0055) acc 74.2188 (68.7211) (mean 62.7321 many 70.8956 med 64.9936 few 50.5698) lr 7.9389e-03 eta 0:07:32
epoch [4/10] batch [30/85] time 0.855 (0.788) data 0.501 (0.448) loss 1.0299 (1.0122) acc 65.6250 (67.8343) (mean 62.1298 many 67.7733 med 66.2141 few 50.7807) lr 7.9389e-03 eta 0:07:25
epoch [4/10] batch [40/85] time 0.760 (0.786) data 0.415 (0.445) loss 1.2463 (1.0615) acc 64.0625 (66.4754) (mean 60.6334 many 65.1696 med 65.9851 few 49.0973) lr 7.9389e-03 eta 0:07:16
epoch [4/10] batch [50/85] time 0.698 (0.786) data 0.373 (0.445) loss 0.9712 (1.0146) acc 65.6250 (67.5868) (mean 60.7239 many 66.0781 med 65.0288 few 49.4549) lr 7.9389e-03 eta 0:07:08
epoch [4/10] batch [60/85] time 0.683 (0.783) data 0.358 (0.443) loss 0.6959 (0.9995) acc 81.2500 (68.0329) (mean 62.5490 many 68.1762 med 66.7222 few 51.1150) lr 7.9389e-03 eta 0:06:58
epoch [4/10] batch [70/85] time 0.698 (0.781) data 0.374 (0.441) loss 0.9921 (0.9732) acc 71.8750 (68.6271) (mean 63.3034 many 68.5457 med 68.3227 few 51.3317) lr 7.9389e-03 eta 0:06:50
epoch [4/10] batch [80/85] time 0.723 (0.779) data 0.399 (0.439) loss 0.9344 (0.9514) acc 74.2188 (69.2423) (mean 63.7005 many 69.0641 med 67.2514 few 53.3003) lr 7.9389e-03 eta 0:06:41
epoch [5/10] batch [10/85] time 0.862 (0.780) data 0.537 (0.440) loss 1.0650 (0.9840) acc 67.9688 (68.3515) (mean 64.7941 many 68.5685 med 70.1295 few 54.1658) lr 6.5451e-03 eta 0:06:30
epoch [5/10] batch [20/85] time 0.683 (0.779) data 0.359 (0.439) loss 0.8570 (0.9194) acc 71.0938 (70.4317) (mean 66.3296 many 69.9361 med 71.4965 few 56.0940) lr 6.5451e-03 eta 0:06:21
epoch [5/10] batch [30/85] time 0.707 (0.777) data 0.382 (0.438) loss 1.2726 (0.9544) acc 62.5000 (68.9588) (mean 64.6258 many 66.4270 med 69.8363 few 56.4455) lr 6.5451e-03 eta 0:06:13
epoch [5/10] batch [40/85] time 0.692 (0.776) data 0.327 (0.436) loss 0.9322 (0.9519) acc 68.7500 (68.6839) (mean 65.7311 many 66.6841 med 71.2047 few 58.2335) lr 6.5451e-03 eta 0:06:04
epoch [5/10] batch [50/85] time 0.708 (0.775) data 0.371 (0.435) loss 1.0145 (0.9811) acc 64.8438 (68.2315) (mean 65.8199 many 69.0242 med 70.0577 few 57.1373) lr 6.5451e-03 eta 0:05:56
epoch [5/10] batch [60/85] time 0.876 (0.774) data 0.541 (0.435) loss 0.9201 (0.9872) acc 71.0938 (67.8972) (mean 65.6393 many 68.1000 med 70.7642 few 56.7895) lr 6.5451e-03 eta 0:05:48
epoch [5/10] batch [70/85] time 0.762 (0.774) data 0.437 (0.435) loss 0.8822 (0.9681) acc 70.3125 (68.5055) (mean 65.2802 many 67.6891 med 68.6347 few 58.5564) lr 6.5451e-03 eta 0:05:40
epoch [5/10] batch [80/85] time 0.870 (0.775) data 0.536 (0.436) loss 1.1241 (0.9576) acc 63.2812 (68.9307) (mean 65.5447 many 66.3109 med 70.5040 few 58.8650) lr 6.5451e-03 eta 0:05:33
epoch [6/10] batch [10/85] time 0.824 (0.774) data 0.482 (0.435) loss 1.1694 (0.9686) acc 64.8438 (67.7870) (mean 65.0316 many 66.9056 med 68.0466 few 59.3278) lr 5.0000e-03 eta 0:05:21
epoch [6/10] batch [20/85] time 0.760 (0.773) data 0.436 (0.434) loss 0.9795 (0.9631) acc 67.9688 (68.6627) (mean 66.2857 many 68.9093 med 68.4372 few 60.7148) lr 5.0000e-03 eta 0:05:12
epoch [6/10] batch [30/85] time 0.842 (0.774) data 0.502 (0.435) loss 1.0749 (0.9286) acc 59.3750 (68.5916) (mean 66.0943 many 65.6449 med 70.7240 few 61.2174) lr 5.0000e-03 eta 0:05:05
epoch [6/10] batch [40/85] time 0.713 (0.774) data 0.388 (0.435) loss 1.0571 (0.9152) acc 69.5312 (69.8073) (mean 67.3869 many 67.9756 med 72.1256 few 61.1715) lr 5.0000e-03 eta 0:04:58
epoch [6/10] batch [50/85] time 0.859 (0.774) data 0.500 (0.435) loss 0.7623 (0.8553) acc 73.4375 (71.0639) (mean 68.1308 many 71.2662 med 70.0164 few 62.2729) lr 5.0000e-03 eta 0:04:50
epoch [6/10] batch [60/85] time 0.697 (0.774) data 0.372 (0.435) loss 0.8601 (0.9110) acc 68.7500 (69.8638) (mean 67.0530 many 69.0315 med 68.1176 few 63.5028) lr 5.0000e-03 eta 0:04:42
epoch [6/10] batch [70/85] time 0.700 (0.773) data 0.375 (0.434) loss 1.0140 (0.9316) acc 60.9375 (69.8679) (mean 66.1370 many 67.3346 med 66.0642 few 64.8245) lr 5.0000e-03 eta 0:04:34
epoch [6/10] batch [80/85] time 0.837 (0.773) data 0.512 (0.434) loss 1.2648 (0.9611) acc 63.2812 (69.0014) (mean 65.6976 many 66.7542 med 67.5487 few 62.3054) lr 5.0000e-03 eta 0:04:26
epoch [7/10] batch [10/85] time 0.753 (0.772) data 0.385 (0.434) loss 0.7993 (0.9169) acc 70.3125 (69.6082) (mean 67.6486 many 70.0250 med 69.8262 few 62.3357) lr 3.4549e-03 eta 0:04:14
epoch [7/10] batch [20/85] time 0.698 (0.771) data 0.374 (0.432) loss 0.7646 (0.9039) acc 73.4375 (70.3726) (mean 67.0936 many 70.4977 med 67.6580 few 62.4636) lr 3.4549e-03 eta 0:04:06
epoch [7/10] batch [30/85] time 0.869 (0.771) data 0.531 (0.433) loss 0.7540 (0.8714) acc 74.2188 (71.9047) (mean 68.4071 many 72.0300 med 70.4232 few 61.8284) lr 3.4549e-03 eta 0:03:59
epoch [7/10] batch [40/85] time 0.851 (0.773) data 0.485 (0.434) loss 0.9005 (0.8821) acc 69.5312 (70.9067) (mean 67.4916 many 70.2274 med 70.8140 few 60.4237) lr 3.4549e-03 eta 0:03:51
epoch [7/10] batch [50/85] time 0.857 (0.774) data 0.533 (0.435) loss 0.5530 (0.8416) acc 78.1250 (71.9281) (mean 68.5213 many 72.0046 med 70.3191 few 62.3600) lr 3.4549e-03 eta 0:03:44
epoch [7/10] batch [60/85] time 0.780 (0.773) data 0.421 (0.435) loss 0.8844 (0.8413) acc 67.9688 (71.7018) (mean 68.7986 many 70.6101 med 72.0099 few 62.9386) lr 3.4549e-03 eta 0:03:36
epoch [7/10] batch [70/85] time 0.683 (0.774) data 0.358 (0.435) loss 0.9429 (0.9062) acc 65.6250 (69.6451) (mean 67.0196 many 67.7263 med 68.4254 few 64.5551) lr 3.4549e-03 eta 0:03:28
epoch [7/10] batch [80/85] time 0.855 (0.773) data 0.487 (0.435) loss 0.8759 (0.9036) acc 72.6562 (70.0407) (mean 68.4852 many 69.6811 med 69.9797 few 65.3463) lr 3.4549e-03 eta 0:03:21
epoch [8/10] batch [10/85] time 0.692 (0.772) data 0.368 (0.434) loss 0.7868 (0.8601) acc 72.6562 (71.3276) (mean 69.1495 many 68.7985 med 71.8418 few 66.4182) lr 2.0611e-03 eta 0:03:09
epoch [8/10] batch [20/85] time 0.749 (0.774) data 0.425 (0.435) loss 0.8301 (0.8788) acc 68.7500 (69.4919) (mean 68.4847 many 69.1940 med 68.6954 few 67.4112) lr 2.0611e-03 eta 0:03:01
epoch [8/10] batch [30/85] time 0.701 (0.774) data 0.376 (0.436) loss 0.8655 (0.8602) acc 68.7500 (70.5132) (mean 69.0795 many 69.5583 med 69.5940 few 67.9207) lr 2.0611e-03 eta 0:02:54
epoch [8/10] batch [40/85] time 0.849 (0.775) data 0.481 (0.436) loss 0.8467 (0.8508) acc 74.2188 (70.7676) (mean 69.3354 many 70.0144 med 70.5372 few 67.1412) lr 2.0611e-03 eta 0:02:46
epoch [8/10] batch [50/85] time 0.682 (0.775) data 0.356 (0.437) loss 0.8229 (0.8399) acc 72.6562 (71.5136) (mean 69.7140 many 70.4669 med 71.4104 few 66.8564) lr 2.0611e-03 eta 0:02:38
epoch [8/10] batch [60/85] time 0.729 (0.775) data 0.405 (0.437) loss 0.7565 (0.8533) acc 71.8750 (72.1144) (mean 69.9022 many 70.5996 med 71.5935 few 67.1156) lr 2.0611e-03 eta 0:02:31
epoch [8/10] batch [70/85] time 0.859 (0.776) data 0.535 (0.437) loss 0.7044 (0.8416) acc 71.8750 (71.6136) (mean 70.4691 many 71.0434 med 73.9192 few 65.7740) lr 2.0611e-03 eta 0:02:23
epoch [8/10] batch [80/85] time 0.837 (0.776) data 0.486 (0.437) loss 0.7513 (0.8588) acc 75.7812 (71.2692) (mean 70.1640 many 71.0867 med 73.3053 few 65.4225) lr 2.0611e-03 eta 0:02:15
epoch [9/10] batch [10/85] time 0.678 (0.775) data 0.318 (0.437) loss 0.8404 (0.8505) acc 71.0938 (71.5481) (mean 70.7252 many 71.7640 med 73.3451 few 66.4567) lr 9.5492e-04 eta 0:02:04
epoch [9/10] batch [20/85] time 0.915 (0.776) data 0.550 (0.437) loss 0.9837 (0.8507) acc 67.9688 (71.4491) (mean 69.7187 many 70.7277 med 70.5446 few 67.5780) lr 9.5492e-04 eta 0:01:56
epoch [9/10] batch [30/85] time 0.822 (0.777) data 0.497 (0.438) loss 0.8407 (0.8281) acc 75.7812 (72.6870) (mean 70.7976 many 74.2589 med 70.8679 few 66.6775) lr 9.5492e-04 eta 0:01:48
epoch [9/10] batch [40/85] time 0.691 (0.776) data 0.366 (0.438) loss 0.7188 (0.8028) acc 74.2188 (73.3657) (mean 71.8162 many 73.2443 med 74.0737 few 67.5164) lr 9.5492e-04 eta 0:01:40
epoch [9/10] batch [50/85] time 0.755 (0.776) data 0.430 (0.438) loss 0.9162 (0.8369) acc 64.8438 (71.6233) (mean 69.9478 many 70.4815 med 71.2053 few 67.8579) lr 9.5492e-04 eta 0:01:33
epoch [9/10] batch [60/85] time 0.840 (0.776) data 0.472 (0.438) loss 1.2589 (0.8872) acc 62.5000 (69.9884) (mean 69.1048 many 67.4636 med 70.9709 few 68.8423) lr 9.5492e-04 eta 0:01:25
epoch [9/10] batch [70/85] time 0.701 (0.776) data 0.375 (0.437) loss 0.7978 (0.8695) acc 74.2188 (71.9185) (mean 70.7434 many 72.1821 med 70.4669 few 69.3874) lr 9.5492e-04 eta 0:01:17
epoch [9/10] batch [80/85] time 0.691 (0.775) data 0.366 (0.437) loss 0.8704 (0.8566) acc 72.6562 (72.0267) (mean 70.2905 many 71.5000 med 70.2132 few 68.9697) lr 9.5492e-04 eta 0:01:09
epoch [10/10] batch [10/85] time 0.682 (0.774) data 0.352 (0.436) loss 0.5929 (0.8404) acc 82.0312 (72.3222) (mean 70.3736 many 73.4061 med 68.5485 few 68.9647) lr 2.4472e-04 eta 0:00:58
epoch [10/10] batch [20/85] time 0.756 (0.774) data 0.393 (0.436) loss 0.6179 (0.8237) acc 76.5625 (72.3913) (mean 71.2995 many 74.2494 med 70.7571 few 68.4908) lr 2.4472e-04 eta 0:00:50
epoch [10/10] batch [30/85] time 0.848 (0.774) data 0.520 (0.436) loss 0.9300 (0.8092) acc 67.9688 (71.8655) (mean 70.0773 many 70.8833 med 71.0747 few 67.9732) lr 2.4472e-04 eta 0:00:42
epoch [10/10] batch [40/85] time 0.693 (0.775) data 0.325 (0.436) loss 0.8855 (0.8381) acc 70.3125 (72.2663) (mean 70.0860 many 72.3303 med 70.0691 few 67.4872) lr 2.4472e-04 eta 0:00:34
epoch [10/10] batch [50/85] time 0.865 (0.775) data 0.540 (0.437) loss 0.8230 (0.8581) acc 71.8750 (72.3396) (mean 70.7690 many 72.9649 med 71.2802 few 67.6106) lr 2.4472e-04 eta 0:00:27
epoch [10/10] batch [60/85] time 0.877 (0.776) data 0.534 (0.438) loss 0.9875 (0.8672) acc 67.1875 (71.2430) (mean 69.8855 many 69.6102 med 71.5820 few 68.2273) lr 2.4472e-04 eta 0:00:19
epoch [10/10] batch [70/85] time 0.930 (0.777) data 0.564 (0.439) loss 0.9662 (0.8751) acc 71.8750 (71.4879) (mean 71.0583 many 69.7254 med 73.3593 few 69.9287) lr 2.4472e-04 eta 0:00:11
epoch [10/10] batch [80/85] time 0.791 (0.778) data 0.424 (0.439) loss 0.7824 (0.7942) acc 74.2188 (72.8238) (mean 72.5816 many 71.3206 med 75.0744 few 71.1446) lr 2.4472e-04 eta 0:00:03
Finish training
Note that the printed training acc is not precise. To get precise training acc, use option ``test_train True``.
Time elapsed: 0:11:08
Checkpoint saved to ./output/cifar100_ir100_clip_vit_b16_adaptformer_True_gpu_0/checkpoint.pth.tar
Evaluate on the test set
=> result
* total: 10,000
* correct: 7,985
* accuracy: 79.8%
* error: 20.2%
* macro_f1: 79.6%
* class acc: [95. 95. 90. 74. 57. 93. 91. 73. 93. 95. 64. 78. 92. 86. 96. 91. 91. 90.
 75. 87. 91. 96. 88. 79. 93. 79. 71. 70. 88. 81. 78. 94. 72. 56. 85. 80.
 92. 86. 78. 89. 86. 86. 75. 89. 76. 73. 87. 45. 98. 92. 64. 85. 82. 95.
 95. 45. 96. 87. 94. 69. 87. 81. 88. 67. 60. 74. 75. 61. 98. 78. 84. 89.
 51. 65. 52. 86. 91. 88. 81. 80. 66. 79. 92. 80. 69. 83. 86. 90. 80. 97.
 90. 79. 58. 65. 88. 62. 33. 74. 43. 64.]
* worst_case_acc: 33.0%
* hmean_acc: 76.6%
* gmean_acc: 78.4%
* many: 83.6%  med: 80.4%  few: 74.8%
* average: 79.8%
